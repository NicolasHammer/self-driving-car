{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "self_driving_notebook.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQlfT9z6-EeH"
      },
      "source": [
        "# Simulated Self-Driving Car in Udacity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8lVBURMlg0q"
      },
      "source": [
        "# Import relevant packages\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy\n",
        "import cv2\n",
        "\n",
        "import csv\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSSscv2W-OYF"
      },
      "source": [
        "## Reading and Splitting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jroM4FBX8aHN"
      },
      "source": [
        "# Read from the log file\n",
        "samples = []\n",
        "with open(\"data/driving_log.csv\") as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  next(reader, None)e\n",
        "  for line in reader:\n",
        "    samples.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJj8TAXX-4Q4"
      },
      "source": [
        "# Divide the data into training set and validation set\n",
        "training_len = int(0.8 * len(samples))\n",
        "valid_len = len(samples) - train_len\n",
        "train_samples, validation_samples,\n",
        " data.random_split(samples, lengths = [train_len, valid_len])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9NNgHg__JFL"
      },
      "source": [
        "# Define the augmentation, transformation process, parameters, and dataset for\n",
        "# dataloader\n",
        "def augment(img_name: str, angle: float):\n",
        "  name = \"data/IMG/\" + imgN_name.split('/')[-1]\n",
        "  current_image = cv2."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}